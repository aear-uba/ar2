# -*- coding: utf-8 -*-
"""actor_critic_onestep_bipedalwalker_01.ipynb

Automatically generated by Colab.

"""

# -----  Algoritmo Actor-Critic aplicado a BipedalWalker  ----- #
#
# ------  UBA - Aprendizaje por Refuerzo II - 2025  ------ #
#

!pip install swig
!pip install gymnasium[box2d]
import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal
from collections import deque
import matplotlib.pyplot as plt
import math
import os # Necesario para crear la carpeta de render
import gymnasium.wrappers # Necesario para RecordVideo

ENV_NAME = "BipedalWalker-v3"
NUM_EPISODES = 5000
MAX_STEPS_PER_EPISODE = 1600
GAMMA = 0.99
HIDDEN_DIM = 256

ACTOR_LR = 1e-5 #1e-4 # 1e-5
CRITIC_LR = 3e-5 #3e-4 # 3e-5

LOG_STD_MIN = -20
LOG_STD_MAX = 2
VIDEO_FOLDER = "videos_bipedal" # Nombre de la carpeta para guardar render

class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, action_low, action_high):
        super(Actor, self).__init__()
        self.action_dim = action_dim
        self.action_low = torch.tensor(action_low, dtype=torch.float32)
        self.action_high = torch.tensor(action_high, dtype=torch.float32)
        self.action_scale = (self.action_high - self.action_low) / 2.0
        self.action_bias = (self.action_low + self.action_high) / 2.0

        self.layer1 = nn.Linear(state_dim, HIDDEN_DIM)
        self.layer2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)
        self.mean_layer = nn.Linear(HIDDEN_DIM, action_dim)
        self.log_std_layer = nn.Linear(HIDDEN_DIM, action_dim)

    def forward(self, state, deterministic=False):
        x = F.relu(self.layer1(state))
        x = F.relu(self.layer2(x))
        mean = self.mean_layer(x)
        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)
        std = torch.exp(log_std)
        dist = Normal(mean, std)

        if deterministic:
            action_unsquashed = mean # Usar la media para acción determinista
        else:
            action_unsquashed = dist.rsample()

        action_squashed = torch.tanh(action_unsquashed)
        scaled_action = action_squashed * self.action_scale + self.action_bias

        log_prob = dist.log_prob(action_unsquashed)

        log_prob = log_prob.sum(axis=-1, keepdim=True)

        return scaled_action, log_prob

class Critic(nn.Module):
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(state_dim, HIDDEN_DIM)
        self.layer2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)
        self.layer3 = nn.Linear(HIDDEN_DIM, 1)

    def forward(self, state):
        x = F.relu(self.layer1(state))
        x = F.relu(self.layer2(x))
        value = self.layer3(x)
        return value

def train_and_record_bipedal():
    # --- Fase de Entrenamiento ---
    env_train = gym.make(ENV_NAME)
    state_dim = env_train.observation_space.shape[0]
    action_dim = env_train.action_space.shape[0]
    action_low = env_train.action_space.low
    action_high = env_train.action_space.high

    actor = Actor(state_dim, action_dim, action_low, action_high)
    critic = Critic(state_dim)
    optimizer_actor = optim.Adam(actor.parameters(), lr=ACTOR_LR)
    optimizer_critic = optim.Adam(critic.parameters(), lr=CRITIC_LR)

    all_episode_rewards = []
    rewards_window = deque(maxlen=100)

    print(f"Iniciando entrenamiento para {NUM_EPISODES} episodios...")
    for episode in range(NUM_EPISODES):
        state, info = env_train.reset()
        state = torch.FloatTensor(state).unsqueeze(0)
        episode_reward = 0
        actor.train() # Asegura que el actor esté en modo entrenamiento
        critic.train()# Asegura que el critico esté en modo entrenamiento

        for step in range(MAX_STEPS_PER_EPISODE):
            action_tensor, log_prob = actor(state, deterministic=False) # Usar acción estocástica
            action_np = action_tensor.detach().cpu().numpy().flatten()

            next_state, reward, terminated, truncated, info = env_train.step(action_np)
            done = terminated or truncated
            episode_reward += reward

            next_state = torch.FloatTensor(next_state).unsqueeze(0)
            reward_tensor = torch.FloatTensor([reward]).unsqueeze(1)
            done_mask = torch.FloatTensor([1.0 - float(done)]).unsqueeze(1)

            value = critic(state)
            with torch.no_grad(): # El target no necesita gradientes que fluyan hacia atrás
                 next_value = critic(next_state)
                 td_target = reward_tensor + GAMMA * next_value * done_mask

            td_error = td_target - value

            critic_loss = F.mse_loss(value, td_target) # td_target ya está detachado implícitamente por no_grad
            optimizer_critic.zero_grad()
            critic_loss.backward()
            # ////////////////////////////////////////
            torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)
            optimizer_critic.step()

            actor_loss = -(log_prob * td_error.detach()).mean() # Detachar td_error aquí
            optimizer_actor.zero_grad()
            actor_loss.backward()
            # ////////////////////////////////////////
            torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)
            optimizer_actor.step()

            state = next_state

            if done:
                break

        all_episode_rewards.append(episode_reward)
        rewards_window.append(episode_reward)
        avg_reward = np.mean(rewards_window)

        if (episode + 1) % 50 == 0:
            print(f'Episodio {episode + 1}/{NUM_EPISODES} | Última Rec: {episode_reward:.2f} | Media (100 ep): {avg_reward:.2f}')
        if avg_reward >= 300 and len(rewards_window) >= 100:
             print(f'Entorno posiblemente resuelto en {episode + 1} episodios! Recompensa media: {avg_reward:.2f}')
             # se puede añadir un break aquí para detener el entrenamiento antes
             # con 5000 iteraciones experimenta aparentemente un suboptimo y en
             # 3000 episodios se estanca

    env_train.close()
    print("Entrenamiento finalizado.")

    # --- Grafica de Resultados del Entrenamiento ---
    plt.figure(figsize=(10, 5))
    plt.plot(all_episode_rewards, label='Recompensa por Episodio', alpha=0.6)
    if len(all_episode_rewards) >= 100:
        moving_avg = np.convolve(all_episode_rewards, np.ones(100)/100, mode='valid')
        plt.plot(np.arange(len(moving_avg)) + 99, moving_avg, label='Media Móvil (100 episodios)', color='red', linewidth=2)
    plt.xlabel("Episodio")
    plt.ylabel("Recompensa Total")
    plt.title(f"Actor-Critic (One-Step) Training Convergence ({ENV_NAME})")
    plt.legend()
    plt.grid(True)
    plt.show() #

    # --- Fase de Grabación de Video render ---
    print(f"\nIniciando grabación de prueba en la carpeta: {VIDEO_FOLDER}")
    os.makedirs(VIDEO_FOLDER, exist_ok=True) # Crea la carpeta si no existe

    # Crea el entorno específicamente para grabar, necesita 'rgb_array'
    env_record = gym.make(ENV_NAME, render_mode='rgb_array')
    env_wrapped = gymnasium.wrappers.RecordVideo(
        env_record,
        video_folder=VIDEO_FOLDER,
        episode_trigger=lambda x: x == 0, # Graba solo el primer episodio de esta instancia
        name_prefix=f"{ENV_NAME}-test"
    )

    actor.eval() # Poner el actor en modo evaluación (importante para dropout, batchnorm, etc.)
    critic.eval() # Aunque no se usa activamente para la acción, es buena práctica

    state, info = env_wrapped.reset()
    done = False
    test_reward = 0
    test_steps = 0

    while not done and test_steps < MAX_STEPS_PER_EPISODE:
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
             # Usar acción determinista (la media) para la prueba suele ser mejor
             action_tensor, _ = actor(state_tensor, deterministic=True)
             action_np = action_tensor.cpu().numpy().flatten()

        next_state, reward, terminated, truncated, info = env_wrapped.step(action_np)
        done = terminated or truncated
        test_reward += reward
        state = next_state
        test_steps += 1

    print(f"Grabación finalizada. Episodio de prueba: Pasos={test_steps}, Recompensa={test_reward:.2f}")
    print(f"El video se ha guardado en la carpeta '{VIDEO_FOLDER}'. Búscalo en el explorador de archivos de Colab.")

    # Importante: cerrar el entorno envuelto para que guarde el video correctamente
    env_wrapped.close()
    env_record.close() # También cerrar el entorno base


if __name__ == '__main__':
    # Ano ollvidarinstalar Box2D
    # En Colab/Linux: !apt-get install swig -y && pip install gymnasium[box2d]
    # En Windows/Mac (puede requerir pasos adicionales): pip install gymnasium[box2d]
    train_and_record_bipedal()